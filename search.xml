<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>数据结构与算法笔记</title>
    <url>/2021/01/02/data-structures-and-algorithms/</url>
    <content><![CDATA[<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2021/01/02/data-structures-and-algorithms/IMG_20201225_124919.jpg">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">或许前路永夜，即便如此我也要前进，因为星光即使微弱也会为我照亮前路。</div>
    <br>
</center>





<a id="more"></a>
<h2 id="一-C-回顾"><a href="#一-C-回顾" class="headerlink" title="一    C++回顾"></a>一    C++回顾</h2><h3 id="1-1-函数和参数"><a href="#1-1-函数和参数" class="headerlink" title="1.1    函数和参数"></a>1.1    函数和参数</h3><p>函数定义时参数表内的参数称为函数的形参，调用函数时使用的参数称为函数的实参。值传递把实参的值复制给形参，使用形参类型的复制构造函数，函数调用返回时形参类型的析构函数负责释放形参。引用传递中形参是实参的别名，形参的变化对应了实参的变化，没有构造函数和析构函数的调用。使用常引用可以使实参的值不被修改。函数的返回值也可以是值、引用和常引用。</p>
<h3 id="1-2-异常"><a href="#1-2-异常" class="headerlink" title="1.2    异常"></a>1.2    异常</h3><p>使用<code>throw</code>和<code>try...catch</code>抛出和处理异常。</p>
<h3 id="1-3-动态内存分配"><a href="#1-3-动态内存分配" class="headerlink" title="1.3    动态内存分配"></a>1.3    动态内存分配</h3><p>使用<code>new</code>来进行动态内存分配，使用<code>delete</code>来释放动态分配的内存。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">make2dArray</span><span class="params">(T**&amp; x,<span class="keyword">int</span> r,<span class="keyword">int</span> c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    x=<span class="keyword">new</span> T* [r];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;r;i++)</span><br><span class="line">        x[i]=<span class="keyword">new</span> T [c];</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">delete2dArray</span><span class="params">(T**&amp; x,<span class="keyword">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;r;i++)</span><br><span class="line">        <span class="keyword">delete</span>[] x[i];</span><br><span class="line">    <span class="keyword">delete</span>[] x;</span><br><span class="line">    x=<span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-4-递归"><a href="#1-4-递归" class="headerlink" title="1.4     递归"></a>1.4     递归</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span><span class="comment">//使用递归生成list[k:m]的所有排列</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">permutations</span><span class="params">(T <span class="built_in">list</span>[], <span class="keyword">int</span> k, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> i;</span><br><span class="line">	<span class="keyword">if</span> (k == m) &#123;</span><br><span class="line">		copy(<span class="built_in">list</span>, <span class="built_in">list</span> + m + <span class="number">1</span>, ostream_iterator&lt;T&gt;(<span class="built_in">cout</span>, <span class="string">&quot;&quot;</span>));</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span>  </span><br><span class="line">		<span class="keyword">for</span> (i = k; i &lt;= m; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			swap(<span class="built_in">list</span>[k], <span class="built_in">list</span>[i]);</span><br><span class="line">			permutations(<span class="built_in">list</span>, k + <span class="number">1</span>, m);</span><br><span class="line">			swap(<span class="built_in">list</span>[k], <span class="built_in">list</span>[i]);</span><br><span class="line">		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-5-标准模板库"><a href="#1-5-标准模板库" class="headerlink" title="1.5    标准模板库"></a>1.5    标准模板库</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;		//计算数组元素的乘积</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;     //copy</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;numeric&gt;       //accumulate</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;    //multiplies</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="function">T <span class="title">product</span><span class="params">(T a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	T theProduct = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">return</span> accumulate(a, a + n, theProduct, multiplies&lt;T&gt;());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span><span class="comment">//使用STL生成数组元素的排列</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">permutations</span><span class="params">(T <span class="built_in">list</span>[], <span class="keyword">int</span> k, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">do</span> &#123;</span><br><span class="line">		copy(<span class="built_in">list</span>, <span class="built_in">list</span> + m + <span class="number">1</span>,</span><br><span class="line">			ostream_iterator&lt;T&gt;(<span class="built_in">cout</span>, <span class="string">&quot;&quot;</span>));</span><br><span class="line">		<span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">	&#125; <span class="keyword">while</span> (next_permutation(<span class="built_in">list</span>, <span class="built_in">list</span> + m + <span class="number">1</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-6-调试和测试"><a href="#1-6-调试和测试" class="headerlink" title="1.6    调试和测试"></a>1.6    调试和测试</h3><p>测试数据分为黑盒法和白盒法。黑盒法要求不同类的数据使得程序结果有质的不同，测试集应当包含每一类的一个输入数据。白盒法要求程序的每条执行路径在测试集中至少被执行一次。</p>
<h2 id="二-程序性能分析和渐进记法"><a href="#二-程序性能分析和渐进记法" class="headerlink" title="二    程序性能分析和渐进记法"></a>二    程序性能分析和渐进记法</h2><h3 id="2-1-渐进记法"><a href="#2-1-渐进记法" class="headerlink" title="2.1    渐进记法"></a>2.1    渐进记法</h3><p>令$p(n)$和$q(n)$是两个非负函数，称$p(n)$渐进地大于$q(n)$当且仅当</p>
<script type="math/tex; mode=display">
\lim_{n\to\infty}\frac{q(n)}{p(n)}=0</script><p>称$q(n)$渐进地小于$p(n)$当且仅当$p(n)$渐进地大于$q(n)$，称$p(n)$渐进地等于$q(n)$当且仅当$q(n)$不渐进地大于$p(n)$且$p(n)$不渐进地大于$q(n)$。</p>
<p>记法$f(n)=O(g(n))$中表示$f(n)$渐进地小于或等于$g(n)$。</p>
<p>记法$f(n)=\Omega(g(n))$中表示$f(n)$渐进地大于或等于$g(n)$。</p>
<p>记法$f(n)=\Theta(g(n))$中表示$f(n)$渐进地等于$g(n)$。</p>
<p>记法$f(n)=o(g(n))$中表示$f(n)$渐进地小于$g(n)$。</p>
<p>$g(n)$通常为系数为1的单项。</p>
<h3 id="2-2-性能测量"><a href="#2-2-性能测量" class="headerlink" title="2.2    性能测量"></a>2.2    性能测量</h3><p>以矩阵乘法为例，在考虑缓存命中与否的情况下，ikj顺序比ijk顺序要快得多，达到了理论时间复杂度，而ijk顺序受缓存未命中影响耗时大大增加。</p>
]]></content>
  </entry>
  <entry>
    <title>利用Tensorflow实现的选课网验证码自动识别AI</title>
    <url>/2021/04/16/PKUAutoEletiveCaptchaHelper/</url>
    <content><![CDATA[<p>前几个星期受朋友的委托，开发了一款针对某大学选课网的验证码识别工具。朋友做的是补退选阶段自动选课部分，而我负责了其中的验证码识别模块。先说一说达到的效果吧，这款工具针对选课网验证码测试集识别率可以达到99.62%，单次识别耗时约0.03秒，实际运行准确率得等到下次选课才知道了，预计不会低于98%。部署在本地可以达到本校学生开发的自动选课工具的平均速度，识别准确度和速度均超越了调用商用验证码识别API。</p>
<a id="more"></a>
<h2 id="图片分析"><a href="#图片分析" class="headerlink" title="图片分析"></a>图片分析</h2><p>没过几天，我就收到了这样一大包带有标签的验证码图片：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2021/04/16/PKUAutoEletiveCaptchaHelper/g8gs_1615517901796011.png">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">g8gs_1615517901796011.png</div>
    <br>
</center>

<p>样本总数是19490个，全部爬取自北京大学选课网，并且在打码平台打好了标签。经过简单的观察和分析，我们可以归纳出验证码的以下几个特征：</p>
<ol>
<li>图片分别率为130*52，png格式。</li>
<li>图片只有一个通道，也就是说这张看起来是彩色的图像实际上是灰度图。</li>
<li>图片名前四个字字符即为对应的标签。</li>
<li>图片存在随机添加的噪点和线条，遮挡关系中均位于底层，因此对识别的影响程度不大。</li>
<li>字符有两种表现形式，一种是实心字符，另一种是空心的边框，对识别影响程度未知。</li>
<li>字符与字符间、字符与图片边界间可能存在遮挡关系，对识别影响程度较大。</li>
<li>验证码字符集是字母和数字的子集，具体来说不包含“0，1，i，j，o，z”这6个字符。</li>
<li>验证码不区分大小写，但图片中字符包含大小写。</li>
<li>同一验证码可能出现重复字符。</li>
<li>标签并非完全准确，后期统计估算的结果是，标签整体准确率约为90%。</li>
</ol>
<p>综合以上几点，我们需要做的是输入一张图片，输出一个字符串，这个字符串即为对该图片的识别结果。很显然这是一个多分类问题，分类总数是$30^4=8.1\times 10^5$。对于这个数量级的多分类问题，我对人工智能能否胜任还打了个问号。毕竟随机一个结果，正确的概率约等于中头奖。面向CSDN编程当然能查出不少别人的成功样例，但是别人的代码和我们目前的需求依然存在不小的差异。我也只能以一点点可怜的tensorflow基础开始写代码。</p>
<h2 id="图像预处理"><a href="#图像预处理" class="headerlink" title="图像预处理"></a>图像预处理</h2><p>tensorflow只能接受张量输入，因此我们首先要做的是把读取的图像转换成tensor类型。与此同时，我们还要从图片名中截出标签字段，转换成tensor一起送入神经网络训练。先把字母表写好：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alphabet = [<span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;8&#x27;</span>, <span class="string">&#x27;9&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;k&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>读入的标签也不能直接喂给模型，需要先转换成独热码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text2vec</span>(<span class="params">text</span>):</span></span><br><span class="line">    vector = np.zeros([<span class="number">4</span>, <span class="number">30</span>])</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(text):</span><br><span class="line">        idx = <span class="built_in">list</span>.index(c)</span><br><span class="line">        vector[i][idx] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> vector</span><br></pre></td></tr></table></figure>
<p>在开始我们的工作前，我们还要把一大包图片分成训练集和测试集（当然keras也支持自动分割），我按照80%：20%的比例划分训练集和测试集。训练集中的损失函数值在反向传播时会更新所有的可训练参数，而测试集仅仅计算损失函数值，相当于预测。训练集和测试集不能有交集，否则达不到训练目的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">path=<span class="string">r&#x27;./img/&#x27;</span></span><br><span class="line"><span class="built_in">list</span>=os.listdir(path)</span><br><span class="line">random.shuffle(<span class="built_in">list</span>)</span><br><span class="line">f=<span class="built_in">open</span>(<span class="string">&#x27;./img.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>:</span><br><span class="line">    f.write(i+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">f.close()</span><br><span class="line">f=<span class="built_in">open</span>(<span class="string">&#x27;./img_test.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>[:<span class="built_in">len</span>(<span class="built_in">list</span>)//<span class="number">5</span>]:</span><br><span class="line">    f.write(i+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">f.close()</span><br><span class="line">f=<span class="built_in">open</span>(<span class="string">&#x27;./img_train.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>[<span class="built_in">len</span>(<span class="built_in">list</span>)//<span class="number">5</span>:]:</span><br><span class="line">    f.write(i+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p>有了这些字母表和函数，我们就可以愉快地往下写读文件的函数了。为了比较模型的稳定性，每次训练我都会随机划分训练集和测试集。我们把图片转换成numpy数组格式并进行归一化，tensorflow可直接读取numpy数组并转换成张量。归一化非常重要，否则你会看到模型loss函数和所有可训练参数直接飞升，通通显示NaN。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateds</span>(<span class="params">path, txt</span>):</span></span><br><span class="line">    f = <span class="built_in">open</span>(txt, <span class="string">&#x27;r&#x27;</span>)  </span><br><span class="line">    contents = f.readlines() </span><br><span class="line">    random.shuffle(contents)</span><br><span class="line">    f.close() </span><br><span class="line">    x, y_ = [], [] </span><br><span class="line">    <span class="keyword">for</span> content <span class="keyword">in</span> contents:</span><br><span class="line">        value = text2vec(content[:<span class="number">4</span>])</span><br><span class="line">        img_path = path + content </span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_path[:-<span class="number">1</span>])</span><br><span class="line">        img = np.array(img) </span><br><span class="line">        img = img / <span class="number">255.</span></span><br><span class="line">        x.append(img) </span><br><span class="line">        y_.append(value)</span><br><span class="line">    x = np.array(x)</span><br><span class="line">    y_ = np.array(y_)</span><br><span class="line">    x = x.astype(np.float32)</span><br><span class="line">    y_ = y_.astype(np.float32) </span><br><span class="line">    x = np.expand_dims(x, axis=<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> x, y_ </span><br></pre></td></tr></table></figure>
<p>当然，实际的代码过程并没有我在这里展示的如此简单，一个极易被忽略的细节是通道。正常的RGB图有三个维度：宽度、高度和通道数。然而这奇葩的验证码竟然没有通道维！傻乎乎的我直接把tensor喂到模型中，报错显示维度不匹配。这小样竟然只有三个维度(None, 52, 130)，而二维卷积需要四个维度！琢磨了半天，我想直接给你加上通道维行不行，于是给图片张量添加了3号维度。竟然行了！这小家伙可是折磨了我整整一个晚上啊。</p>
<p>另外值得注意的是，二维卷积需求的张量维度分别是batch数、高度、宽度和通道数。碰巧numpy数组也是高度维在前，宽度维在后，如果反过来的话也很容易中招。</p>
<h2 id="利用keras搭建卷积神经网络"><a href="#利用keras搭建卷积神经网络" class="headerlink" title="利用keras搭建卷积神经网络"></a>利用keras搭建卷积神经网络</h2><p>keras已经把tensorflow的方法封装得相当到位了，搭建网络的过程并没有给我造成太大的困难。我先比较了一下常见的CNN代码实现难度，VGGNet算是最好写的，不用写类就能搞得定，而且代码结构也很规整。如果识别水平实在不行，那就再试试别的，比如InceptionNet或是ResNet。</p>
<p>我对VGGNet做了一些调整，在这里卷积神经网络基本结构如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Model: &quot;model&quot;</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to                     </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">input_1 (InputLayer)            [(None, 52, 130, 1)] 0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d (Conv2D)                 (None, 52, 130, 32)  320         input_1[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization (BatchNorma (None, 52, 130, 32)  128         conv2d[0][0]                     </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation (Activation)         (None, 52, 130, 32)  0           batch_normalization[0][0]        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)               (None, 52, 130, 32)  9248        activation[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_1 (BatchNor (None, 52, 130, 32)  128         conv2d_1[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_1 (Activation)       (None, 52, 130, 32)  0           batch_normalization_1[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D)    (None, 26, 65, 32)   0           activation_1[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout (Dropout)               (None, 26, 65, 32)   0           max_pooling2d[0][0]              </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)               (None, 26, 65, 64)   18496       dropout[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_2 (BatchNor (None, 26, 65, 64)   256         conv2d_2[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_2 (Activation)       (None, 26, 65, 64)   0           batch_normalization_2[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_3 (Conv2D)               (None, 26, 65, 64)   36928       activation_2[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_3 (BatchNor (None, 26, 65, 64)   256         conv2d_3[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_3 (Activation)       (None, 26, 65, 64)   0           batch_normalization_3[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2D)  (None, 13, 33, 64)   0           activation_3[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)             (None, 13, 33, 64)   0           max_pooling2d_1[0][0]            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_4 (Conv2D)               (None, 13, 33, 128)  73856       dropout_1[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_4 (BatchNor (None, 13, 33, 128)  512         conv2d_4[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_4 (Activation)       (None, 13, 33, 128)  0           batch_normalization_4[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_5 (Conv2D)               (None, 13, 33, 128)  147584      activation_4[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_5 (BatchNor (None, 13, 33, 128)  512         conv2d_5[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_5 (Activation)       (None, 13, 33, 128)  0           batch_normalization_5[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_2 (MaxPooling2D)  (None, 7, 17, 128)   0           activation_5[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)             (None, 7, 17, 128)   0           max_pooling2d_2[0][0]            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_6 (Conv2D)               (None, 7, 17, 256)   295168      dropout_2[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_6 (BatchNor (None, 7, 17, 256)   1024        conv2d_6[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_6 (Activation)       (None, 7, 17, 256)   0           batch_normalization_6[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_7 (Conv2D)               (None, 7, 17, 256)   590080      activation_6[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_7 (BatchNor (None, 7, 17, 256)   1024        conv2d_7[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_7 (Activation)       (None, 7, 17, 256)   0           batch_normalization_7[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_3 (MaxPooling2D)  (None, 4, 9, 256)    0           activation_7[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_3 (Dropout)             (None, 4, 9, 256)    0           max_pooling2d_3[0][0]            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_8 (Conv2D)               (None, 4, 9, 512)    1180160     dropout_3[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_8 (BatchNor (None, 4, 9, 512)    2048        conv2d_8[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_8 (Activation)       (None, 4, 9, 512)    0           batch_normalization_8[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">conv2d_9 (Conv2D)               (None, 4, 9, 512)    2359808     activation_8[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">batch_normalization_9 (BatchNor (None, 4, 9, 512)    2048        conv2d_9[0][0]                   </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation_9 (Activation)       (None, 4, 9, 512)    0           batch_normalization_9[0][0]      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_4 (MaxPooling2D)  (None, 2, 5, 512)    0           activation_9[0][0]               </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_4 (Dropout)             (None, 2, 5, 512)    0           max_pooling2d_4[0][0]            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten (Flatten)               (None, 5120)         0           dropout_4[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense (Dense)                   (None, 4096)         20975616    flatten[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_5 (Dropout)             (None, 4096)         0           dense[0][0]                      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_1 (Dense)                 (None, 4096)         16781312    dropout_5[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_6 (Dropout)             (None, 4096)         0           dense_1[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_2 (Dense)                 (None, 30)           122910      dropout_6[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_3 (Dense)                 (None, 30)           122910      dropout_6[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_4 (Dense)                 (None, 30)           122910      dropout_6[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_5 (Dense)                 (None, 30)           122910      dropout_6[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf.convert_to_tensor (TFOpLambd (4, None, 30)        0           dense_2[0][0]                    </span><br><span class="line">                                                                 dense_3[0][0]                    </span><br><span class="line">                                                                 dense_4[0][0]                    </span><br><span class="line">                                                                 dense_5[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf.compat.v1.transpose (TFOpLam (None, 4, 30)        0           tf.convert_to_tensor[0][0]       </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 42,968,152</span><br><span class="line">Trainable params: 42,964,184</span><br><span class="line">Non-trainable params: 3,968</span><br><span class="line">__________________________________________________________________________________________________</span><br></pre></td></tr></table></figure>
<p>第一部分是10层卷积。使用3*3的卷积核，卷积核个数从32开始，每两层变成原来的两倍，与此同时深度也会变为原来的两倍。卷积步长为1，使用全零填充。接着进行批标准化操作，使用relu激活函数。每两层我们做一次2*2的最大池化，池化步长为2，使用全零填充。每两层我们舍弃一次，每层舍弃20%的节点以防止模型过拟合。</p>
<p>第二部分是两层4096个节点的全连接，全连接前先把张量拉直成一维，采用relu激活函数。最后一步是连接到输出层，这里我们使用了4个全连接层代表了输出的四个字符，在这里我们使用softmax激活函数使输出符合概率分布。为了匹配最终的输出结果，我们还要调换一下输出张量的维度顺序。</p>
<p>最后的模型代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line">input_tensor = Input((<span class="number">52</span>, <span class="number">130</span>, <span class="number">1</span>))</span><br><span class="line">x = input_tensor</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = Convolution2D(filters=<span class="number">32</span> * <span class="number">2</span> ** (i // <span class="number">2</span>), kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    x = BatchNormalization()(x)</span><br><span class="line">    x = Activation(activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        x = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">        x = Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">x = Flatten()(x)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    x = Dense(<span class="number">4096</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">x = tf.convert_to_tensor([Dense(<span class="number">30</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(x) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">x = tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h2 id="训练卷积神经网络"><a href="#训练卷积神经网络" class="headerlink" title="训练卷积神经网络"></a>训练卷积神经网络</h2><p>炼丹炉已经造好，原料也已准备完毕，现在终于可以开始“炼丹”了。加载模型，设置好优化器，设置好损失函数和准确度，设置好存档和断点续训，设置好batch大小和epoch，设置好回调函数，喂入数据和标签，现在开始听天由命了，我也不知道这台机器能学成什么鬼样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.callbacks <span class="keyword">import</span> *</span><br><span class="line">train_path = <span class="string">&#x27;./img/&#x27;</span></span><br><span class="line">train_txt = <span class="string">&#x27;./venv/img_train.txt&#x27;</span></span><br><span class="line">test_path = <span class="string">&#x27;./img/&#x27;</span></span><br><span class="line">test_txt = <span class="string">&#x27;./img_test.txt&#x27;</span></span><br><span class="line">x_train, y_train = generateds(train_path, train_txt)</span><br><span class="line">x_test, y_test = generateds(test_path, test_txt)</span><br><span class="line">model = Model(input_tensor, x)</span><br><span class="line">optimizer = Adadelta(learning_rate=<span class="number">0.8</span>)</span><br><span class="line">reduce_lr = LearningRateScheduler(scheduler)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,optimizer=optimizer,metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">checkpoint_save_path = <span class="string">&quot;./captcha.ckpt&quot;</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;-------------load the model-----------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line">cp_callback = ModelCheckpoint(filepath=checkpoint_save_path,save_weights_only=<span class="literal">True</span>, save_best_only=<span class="literal">True</span>)</span><br><span class="line">history = model.fit(x_train, y_train, batch_size=<span class="number">64</span>, epochs=<span class="number">20</span>, validation_data=(x_test, y_test), validation_freq=<span class="number">1</span>,callbacks=[cp_callback, reduce_lr])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>训了几个小时，我就发现有点不对劲了。loss函数和准确度几乎没有收敛的意思，仍在瞎蒙附近徘徊。这事就有点诡异了，按理说是会收敛的，难道是我的学习率还不够大？我的学习率是默认的0.01，这么复杂的模型，光是训练一轮就要40s，如果调大学习率恐怕会出现不收敛的情况。保险起见，我让它以低学习率通宵训练，看看800个epoch后是个什么样。</p>
<p>第二天早上起来，炼丹的结果让我喜出望外。一个晚上的风扇轰鸣没有白费，虽然开始的100个epoch的确是瞎蒙，但后来神经网络似乎找到了门道，准确度开始线性增长。单字符准确度从0.03增加到0.7，现在神经网络已经从人工智障逐步训练成人工智能了，但离应用还天远地远。4个字符组成的验证码准确度是单字符的4次方，也就是$0.7^4=0.2401$，从应用角度来看和完全不能用也没什么区别。也就是说，学到了，但没有完全学到。</p>
<p>接下来要做的事就简单了，我们仅需可劲地提高学习率。当然，高学习率也就导致了模型在短短10个epoch后准确率就达到90%，此后模型迅速出现过拟合现象。为此，我们需要设置指数衰减学习率，也就有了以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.backend <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scheduler</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">4</span> == <span class="number">0</span> <span class="keyword">and</span> epoch != <span class="number">0</span>:</span><br><span class="line">        lr = get_value(model.optimizer.lr)</span><br><span class="line">        set_value(model.optimizer.lr, lr * <span class="number">0.5</span>)</span><br><span class="line">        print(<span class="string">&quot;lr changed to &#123;&#125;&quot;</span>.<span class="built_in">format</span>(lr * <span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> get_value(model.optimizer.lr)</span><br></pre></td></tr></table></figure>
<p>为了直观地显示训练进度，可以使用matplotlib绘制acc曲线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">acc = history.history[<span class="string">&#x27;accuracy&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_accuracy&#x27;</span>]</span><br><span class="line">acc = [i ** <span class="number">4</span> <span class="keyword">for</span> i <span class="keyword">in</span> acc]</span><br><span class="line">val_acc = [i ** <span class="number">4</span> <span class="keyword">for</span> i <span class="keyword">in</span> val_acc]</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(acc, label=<span class="string">&#x27;Training Accuracy&#x27;</span>)</span><br><span class="line">plt.plot(val_acc, label=<span class="string">&#x27;Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and Validation Accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="优化识别准确率"><a href="#优化识别准确率" class="headerlink" title="优化识别准确率"></a>优化识别准确率</h2><p>在接下来的几个小时里，通过不断调整模型参数，单字符准确度从0.7迅速增加到0.95，训练一次模型的时间也从30分钟缩短到15分钟。然而，$0.95^4\approx0.815$，这依然达不到可应用的水平。现在我可以一个个的看看模型和标签不一样的地方在哪里了。我们先写个译码函数把输出张量转换成字符串：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">y</span>):</span></span><br><span class="line">    y = np.argmax(y, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([alphabet[x] <span class="keyword">for</span> x <span class="keyword">in</span> y])</span><br></pre></td></tr></table></figure>
<p>把所有图片一起喂进去，和标签不一样的全部打印出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_ans, y_ans = generateds(train_path, <span class="string">&#x27;./venv/img.txt&#x27;</span>)</span><br><span class="line">y_pred = model.predict(x_ans)</span><br><span class="line">ans = []</span><br><span class="line">pred = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y_ans:</span><br><span class="line">    ans.append(decode(i))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> y_pred:</span><br><span class="line">    pred.append(decode(i))</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ans)):</span><br><span class="line">    <span class="keyword">if</span> ans[i] != pred[i]:</span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(j)</span><br><span class="line">        print(ans[i], <span class="string">&#x27;  &#x27;</span>, pred[i])</span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">print(j)</span><br></pre></td></tr></table></figure>
<p>好家伙，打印出1500多个！</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">sgan    sgam</span><br><span class="line">mam7    mamv</span><br><span class="line">tfsm    tfsn</span><br><span class="line">5mdh    4mdh</span><br><span class="line">d8fr    dbfr</span><br><span class="line">dcsb    d2sb</span><br><span class="line">htvn    htvm</span><br><span class="line">qm46    qh46</span><br><span class="line">cwpa    gwpa</span><br><span class="line">glmf    glmt</span><br><span class="line">wm4e    wmhe</span><br><span class="line">hapl    hbpl</span><br><span class="line">srgk    spgk</span><br><span class="line">mxms    mxm5</span><br><span class="line">bums    bum5</span><br><span class="line">twut    twu7</span><br><span class="line">apvn    apvh</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>点进去一个，一看是标签错了。又点进去一个，又是标签错了……一个个看下来有八成都是标签的错。看来我的神经网络也没有那么不堪嘛。修改标签的这部分工作实在是令人难以忍受，我足足花了几乎两个整天才把1500多个验证码一一人工看过一遍。当然，蛮干是不可能的，最好的办法是一边修改标签一边重新训练模型提高准确度。后来的训练结果如下，仅仅训练20轮准确率就达到99%以上，此时甚至还在欠拟合区。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Epoch 1&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 38s 140ms&#x2F;step - loss: 4.0398 - accuracy: 0.0437 - val_loss: 3.3373 - val_accuracy: 0.0412</span><br><span class="line">Epoch 2&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 31s 128ms&#x2F;step - loss: 3.2931 - accuracy: 0.0583 - val_loss: 3.7456 - val_accuracy: 0.0402</span><br><span class="line">Epoch 3&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 31s 129ms&#x2F;step - loss: 2.9358 - accuracy: 0.1276 - val_loss: 3.1022 - val_accuracy: 0.1287</span><br><span class="line">Epoch 4&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 129ms&#x2F;step - loss: 2.1117 - accuracy: 0.3075 - val_loss: 1.4998 - val_accuracy: 0.4837</span><br><span class="line">Epoch 5&#x2F;20</span><br><span class="line">lr changed to 0.4000000059604645</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.9710 - accuracy: 0.6563 - val_loss: 0.8676 - val_accuracy: 0.7014</span><br><span class="line">Epoch 6&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.5527 - accuracy: 0.8030 - val_loss: 0.2733 - val_accuracy: 0.9088</span><br><span class="line">Epoch 7&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 129ms&#x2F;step - loss: 0.3350 - accuracy: 0.8831 - val_loss: 0.1302 - val_accuracy: 0.9582</span><br><span class="line">Epoch 8&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.2138 - accuracy: 0.9277 - val_loss: 0.1094 - val_accuracy: 0.9643</span><br><span class="line">Epoch 9&#x2F;20</span><br><span class="line">lr changed to 0.20000000298023224</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.1041 - accuracy: 0.9646 - val_loss: 0.0466 - val_accuracy: 0.9863</span><br><span class="line">Epoch 10&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.0779 - accuracy: 0.9747 - val_loss: 0.0421 - val_accuracy: 0.9890</span><br><span class="line">Epoch 11&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.0599 - accuracy: 0.9795 - val_loss: 0.0376 - val_accuracy: 0.9902</span><br><span class="line">Epoch 12&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 129ms&#x2F;step - loss: 0.0532 - accuracy: 0.9824 - val_loss: 0.0319 - val_accuracy: 0.9919</span><br><span class="line">Epoch 13&#x2F;20</span><br><span class="line">lr changed to 0.10000000149011612</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.0376 - accuracy: 0.9876 - val_loss: 0.0323 - val_accuracy: 0.9927</span><br><span class="line">Epoch 14&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.0306 - val_accuracy: 0.9931</span><br><span class="line">Epoch 15&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 130ms&#x2F;step - loss: 0.0283 - accuracy: 0.9905 - val_loss: 0.0271 - val_accuracy: 0.9942</span><br><span class="line">Epoch 16&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 32s 132ms&#x2F;step - loss: 0.0259 - accuracy: 0.9906 - val_loss: 0.0254 - val_accuracy: 0.9947</span><br><span class="line">Epoch 17&#x2F;20</span><br><span class="line">lr changed to 0.05000000074505806</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 35s 145ms&#x2F;step - loss: 0.0221 - accuracy: 0.9922 - val_loss: 0.0235 - val_accuracy: 0.9949</span><br><span class="line">Epoch 18&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 35s 145ms&#x2F;step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.0258 - val_accuracy: 0.9944</span><br><span class="line">Epoch 19&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 36s 146ms&#x2F;step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.0241 - val_accuracy: 0.9951</span><br><span class="line">Epoch 20&#x2F;20</span><br><span class="line">244&#x2F;244 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 35s 145ms&#x2F;step - loss: 0.0197 - accuracy: 0.9933 - val_loss: 0.0234 - val_accuracy: 0.9951</span><br></pre></td></tr></table></figure>
<p>可视化结果如下：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2021/04/16/PKUAutoEletiveCaptchaHelper/final.png">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;"></div>
    <br>
</center>

<p>最后一次训练和标签不一致的样本数是75个，准确率达到99.62%。我们随便找个不一样的看看：</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/2021/04/16/PKUAutoEletiveCaptchaHelper/glmf_1615512501114772.png">
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    "></div>
    <br>
</center>

<p>AI认为它是“glmf”，还是挺阴间的对吧？</p>
<h2 id="封装识别模块"><a href="#封装识别模块" class="headerlink" title="封装识别模块"></a>封装识别模块</h2><p>至此，我们的炼丹算是全部完成了，最后生成的模型文件有491MB。我们把模型的预测部分单独拎出来，封装成一个识别函数，输入图片路径返回字符串。把加载神经网络的部分封装成另一个函数，在主程序开始时调用以加载神经网络。实测加载神经网络大约需要1秒，每调用一次识别函数大约需要0.03秒。在识别函数处我预留了同时识别多张图片的接口，但目前还看不到有此需求。实际上由于并行计算的特性，一次识别不太多的多张图片所花的时间几乎相同。验证码识别部分的完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在主程序开始时调用此函数加载神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init</span>():</span></span><br><span class="line">    <span class="comment"># 输入层</span></span><br><span class="line">    input_tensor = Input((<span class="number">52</span>, <span class="number">130</span>, <span class="number">1</span>))</span><br><span class="line">    x = input_tensor</span><br><span class="line">    <span class="comment"># 10层卷积，每层批标准化，采用relu激活，每两层进行一次最大池化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        x = Convolution2D(filters=<span class="number">32</span> * <span class="number">2</span> ** (i // <span class="number">2</span>), kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">        x = BatchNormalization()(x)</span><br><span class="line">        x = Activation(activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            x = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">&#x27;same&#x27;</span>)(x)</span><br><span class="line">    <span class="comment"># 拉直成一维送入全连接</span></span><br><span class="line">    x = Flatten()(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        x = Dense(<span class="number">4096</span>, activation=<span class="string">&#x27;relu&#x27;</span>)(x)</span><br><span class="line">    <span class="comment"># 输出层</span></span><br><span class="line">    x = tf.convert_to_tensor([Dense(<span class="number">30</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)(x) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)])</span><br><span class="line">    x = tf.transpose(x, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">    <span class="comment"># 加载神经网络</span></span><br><span class="line">    model = Model(input_tensor, x)</span><br><span class="line">    checkpoint = <span class="string">&quot;./captcha.ckpt&quot;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(checkpoint + <span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">        model.load_weights(checkpoint)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 识别验证码，参数为图片路径和模型，返回值为识别结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">path, model</span>):</span></span><br><span class="line">    alphabet = [<span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;6&#x27;</span>, <span class="string">&#x27;7&#x27;</span>, <span class="string">&#x27;8&#x27;</span>, <span class="string">&#x27;9&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;k&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>]</span><br><span class="line">    <span class="comment"># 读取图片，预处理</span></span><br><span class="line">    x_arr = []</span><br><span class="line">    img = Image.<span class="built_in">open</span>(path)</span><br><span class="line">    img = np.array(img)</span><br><span class="line">    img = img / <span class="number">255.</span></span><br><span class="line">    x_arr.append(img)</span><br><span class="line">    <span class="comment"># 转化为np_array并添加通道维</span></span><br><span class="line">    x_arr = np.array(x_arr)</span><br><span class="line">    x_arr = x_arr.astype(np.float32)</span><br><span class="line">    x_arr = np.expand_dims(x_arr, axis=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 识别结果并输出</span></span><br><span class="line">    y_arr = model.predict(x_arr)</span><br><span class="line">    pred = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> y_arr:</span><br><span class="line">        n = np.argmax(i, axis=<span class="number">1</span>)</span><br><span class="line">        pred.append(<span class="string">&#x27;&#x27;</span>.join([alphabet[j] <span class="keyword">for</span> j <span class="keyword">in</span> n]))</span><br><span class="line">    <span class="keyword">return</span> pred[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>从网上选课出现的第一天起，自动选课作为游走在规定和监管边缘的存在一次次地掀起选课学生的讨论，乃至争执。从远古时期的鼠标宏，发展到现今利用CNN实现验证码识别，自动选课技术似乎永远快了选课网一步。无论“加剧内卷”与否，自动选课工具本身就是极大解放学生精力的存在，这点无可非议。而且从自动选课工具的实现原理来看，它的特点之一就是和正常访问选课网的行为无异。也就是说，选课网无法分辨出某一请求是来自于真人，还是自动选课工具。验证码——全称“全自动区分计算机和人类的图灵测试”——在卷积神经网络面前也不堪一击。事实上，当前所有可用的自动选课工具都是开源的，只有用与不用的区别，和工具本身无关。当下能做的只有期待更合理的选课制度在不远的将来来到我们身边。</p>
]]></content>
  </entry>
</search>
